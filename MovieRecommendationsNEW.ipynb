{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recommendation System with Spark ML\n",
    "\n",
    "In this notebook we explore creating a movie recommendation system using Spark ML. We will work with the MovieLens 1M dataset (http://grouplens.org/datasets/movielens). This dataset consists of more than 1 million ratings of approximately 4000 movies made by 6000 MovieLens users who joined MovieLens in 2000. MovieLens is a recommender system and virtual community website that recommends movies for its users to watch, based on their film preferences using collaborative filtering. This benchmark dataset was released February 2003.\n",
    "\n",
    "A common task of recommender systems is to improve customer experience through personalized recommendations based on prior user feedback. Collaborative filtering is a technique that is commonly used for recommender systems. It employs a form of wisdom of the crowd approach to generate recommendations based on the preferences of users.\n",
    "\n",
    "Spark ML supports an implementation of matrix factorization for collaborative filtering. Matrix factorization models have consistently shown to perform extremely well for collaborative filtering. The type of matrix factorization we will explore in this notebook is called explicit matrix factorization. In explicit matrix factorization, preferences provided by users themselves are utilized - as contrasted with implicit matrix factorization, where only implicit feedback (e.g. views, clicks, purchases, likes, shares etc.) is utilized. Collaborative filtering aims to fill in the missing entries of a user-item (in the case of movie recommendations, this consists of user and movie IDs) association matrix in which users (userID) and items (movieID) are described by a small set of latent factors that can be used to predict missing entries. Spark ML uses the Alternating Least Squares (ALS) algorithm to learn these latent factors for this matix factorization problem. ALS works by iteratively solving a series of least square regression problems to derive a model.\n",
    "\n",
    "![Factorization Graphic](https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcSID6SakBUeYVGD4VUJ06oJwnEtqeXfnicgBWu5n7fIDTY6HsHooA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please note that this notebook requires Spark 1.6.0 or greater\n",
    "See JIRA SPARK-11284. This notebook was written using Spark 1.6.1, but will run unchanged on Spark 2.0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Spark version and existence of Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version = 2.1.2\n"
     ]
    }
   ],
   "source": [
    "println(\"Spark version = \" + sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Spark libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.recommendation.ALS\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc); import sqlContext.implicits._\n",
    "import sys.process._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the MovieLens 1M dataset from  http://grouplens.org/datasets/movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-04-23 21:44:56--  http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.34.235\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.34.235|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5917549 (5.6M) [application/zip]\n",
      "Saving to: ‘ml-1m.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  917K 6s\n",
      "    50K .......... .......... .......... .......... ..........  1% 1.80M 5s\n",
      "   100K .......... .......... .......... .......... ..........  2% 42.7M 3s\n",
      "   150K .......... .......... .......... .......... ..........  3% 65.1M 2s\n",
      "   200K .......... .......... .......... .......... ..........  4% 1.87M 2s\n",
      "   250K .......... .......... .......... .......... ..........  5% 92.7M 2s\n",
      "   300K .......... .......... .......... .......... ..........  6%  104M 2s\n",
      "   350K .......... .......... .......... .......... ..........  6%  109M 1s\n",
      "   400K .......... .......... .......... .......... ..........  7% 1.89M 2s\n",
      "   450K .......... .......... .......... .......... ..........  8%  106M 1s\n",
      "   500K .......... .......... .......... .......... ..........  9%  102M 1s\n",
      "   550K .......... .......... .......... .......... .......... 10% 99.6M 1s\n",
      "   600K .......... .......... .......... .......... .......... 11%  108M 1s\n",
      "   650K .......... .......... .......... .......... .......... 12% 99.5M 1s\n",
      "   700K .......... .......... .......... .......... .......... 12% 98.9M 1s\n",
      "   750K .......... .......... .......... .......... .......... 13%  103M 1s\n",
      "   800K .......... .......... .......... .......... .......... 14% 2.04M 1s\n",
      "   850K .......... .......... .......... .......... .......... 15% 98.1M 1s\n",
      "   900K .......... .......... .......... .......... .......... 16%  123M 1s\n",
      "   950K .......... .......... .......... .......... .......... 17%  108M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 18%  113M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 19%  113M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 19%  113M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 20%  119M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 21%  111M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 22% 95.6M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 23%  117M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 24%  103M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 25%  108M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 25%  115M 0s\n",
      "  1500K .......... .......... .......... .......... .......... 26% 98.3M 0s\n",
      "  1550K .......... .......... .......... .......... .......... 27%  122M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 28% 2.37M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 29%  104M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 30% 95.1M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 31% 88.6M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 32%  127M 0s\n",
      "  1850K .......... .......... .......... .......... .......... 32% 97.0M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 33% 97.6M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 34%  101M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 35%  106M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 36%  120M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 37%  114M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 38%  108M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 38%  101M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 39%  119M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 40%  119M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 41% 99.7M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 42%  102M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 43% 96.3M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 44%  107M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 44%  110M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 45%  103M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 46%  119M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 47%  113M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 48%  111M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 49%  116M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 50% 96.6M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 51%  125M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 51%  122M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 52%  100M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 53%  103M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 54%  121M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 55%  119M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 56%  110M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 57% 3.72M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 57% 92.3M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 58%  112M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 59%  120M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 60%  116M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 61%  109M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 62% 96.9M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 63%  130M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 64%  116M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 64% 97.1M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 65%  104M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 66% 62.4M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 67%  308M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 68%  132M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 69%  120M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 70% 99.1M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 70%  131M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 71%  108M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 72%  116M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 73%  109M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 74%  108M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 75%  112M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 76%  102M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 77%  119M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 77%  118M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 78%  110M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 79%  102M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 80%  128M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 81%  105M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 82%  110M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 83%  111M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 83%  121M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 84%  110M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 85%  113M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 86%  101M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 87%  118M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 88%  110M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 89%  113M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 89%  113M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 90%  106M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 91%  110M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 92% 5.29M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 93% 53.7M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 94% 85.2M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 95% 96.8M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 96% 94.0M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 96%  104M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 97% 96.9M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 98% 99.9M 0s\n",
      "  5700K .......... .......... .......... .......... .......... 99%  120M 0s\n",
      "  5750K .......... .......... ........                        100% 88.4M=0.3s\n",
      "\n",
      "2018-04-23 21:44:56 (22.5 MB/s) - ‘ml-1m.zip’ saved [5917549/5917549]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"rm -f ./ml-1m.zip\".!\n",
    "\"wget http://files.grouplens.org/datasets/movielens/ml-1m.zip\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show that the MovieLens 1M dataset zip file is now on the local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ml-1m.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ls ./ml-1m.zip\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip the MovieLens 1M dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ml-1m.zip\n",
      "   creating: ml-1m/\n",
      "  inflating: ml-1m/movies.dat        \n",
      "  inflating: ml-1m/ratings.dat       \n",
      "  inflating: ml-1m/README            \n",
      "  inflating: ml-1m/users.dat         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"rm -r ./ml-1m\".!\n",
    "\"unzip ml-1m.zip\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the unzipped MovieLens 1M dataset files\n",
    "### In this demo we will only be using the \"ratings.dat\" and \"movies.dat\" dataset. We will not use the \"users.dat\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies.dat\n",
      "ratings.dat\n",
      "README\n",
      "users.dat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ls ./ml-1m\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Ratings Dataset\n",
    "### Ratings File Description\n",
    "\n",
    "All ratings are contained in the file “ratings.dat” and are in the following format:\n",
    "\n",
    "    UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "    UserIDs range between 1 and 6040\n",
    "    MovieIDs range between 1 and 3952\n",
    "    Ratings are made on a 5-star scale (whole-star ratings only)\n",
    "    Timestamp is represented in seconds since the epoch (1 January 1970)\n",
    "    \n",
    "Each user has at least 20 ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read contents of \"ratings.dat\" and show sample content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, IntegerType, FloatType, StringType}\n",
    "val ratingsSchema = StructType(Array(StructField(\"userId\", StringType, true), StructField(\"movieId\", StringType, true), StructField(\"rating\", StringType, true)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ratings_raw = sc.textFile(\"./ml-1m/ratings.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "val ratings_RDD = ratings_raw.map(line => Row.fromSeq(line.split(\"::\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ratings data to a DataFrame\n",
    "Don't need the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ratings_df = spark.createDataFrame(ratings_RDD, ratingsSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "val ratings = ratings_df.withColumn(\"userIdTemp\", ratings_df(\"userId\").cast(IntegerType)).drop(\"userId\").withColumnRenamed(\"userIdTemp\", \"userId\").withColumn(\"movieIdTemp\", ratings_df(\"movieId\").cast(IntegerType)).drop(\"movieId\").withColumnRenamed(\"movieIdTemp\", \"movieId\").withColumn(\"ratingTemp\", ratings_df(\"rating\").cast(FloatType)).drop(\"rating\").withColumnRenamed(\"ratingTemp\", \"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the number of ratings in the dataset is slightly more than one million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ratings = 1000209                                                     \n"
     ]
    }
   ],
   "source": [
    "println(\"Number of ratings = \" + ratings_raw.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a sample of the Ratings DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|    68|   2908|   5.0|\n",
      "|   173|   3730|   5.0|\n",
      "|   456|   2917|   2.0|\n",
      "|   526|    589|   4.0|\n",
      "|   533|   2348|   3.0|\n",
      "|   588|   1285|   4.0|\n",
      "|   711|   1206|   4.0|\n",
      "|   730|   3361|   4.0|\n",
      "|   779|   3203|   5.0|\n",
      "|   843|   1196|   4.0|\n",
      "+------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.sample(false, 0.0001, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show sample number of ratings per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+                                                         \n",
      "|userId|No. of ratings|\n",
      "+------+--------------+\n",
      "|   148|           624|\n",
      "|   463|           123|\n",
      "|   471|           105|\n",
      "|   496|           119|\n",
      "|   833|            21|\n",
      "|  1088|          1176|\n",
      "|  1238|            45|\n",
      "|  1342|            92|\n",
      "|  1580|            37|\n",
      "|  1591|           314|\n",
      "+------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val grouped_ratings = ratings.groupBy(\"userId\").count().withColumnRenamed(\"count\", \"No. of ratings\")\n",
    "grouped_ratings.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the number of users in the dataset is approximately 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 6040                                                          \n"
     ]
    }
   ],
   "source": [
    "println(\"Number of users = \" + grouped_ratings.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Movies Dataset\n",
    "### Movies File Description\n",
    "\n",
    "Movie information is in the file “movies.dat” and is in the following format:\n",
    "\n",
    "    movieId::Title::Genres\n",
    "\n",
    "Titles are identical to titles provided by the IMDb and include the year of release\n",
    "\n",
    "Genres are pipe-separated and are selected from the following genres:\n",
    "* Action\n",
    "* Adventure\n",
    "* Animation\n",
    "* Children's\n",
    "* Comedy\n",
    "* Crime\n",
    "* Documentary\n",
    "* Drama\n",
    "* Fantasy\n",
    "* Film-Noir\n",
    "* Horror\n",
    "* Musical\n",
    "* Mystery\n",
    "* Romance\n",
    "* Sci-Fi\n",
    "* Thriller\n",
    "* War\n",
    "* Western\n",
    "\n",
    "Some MovieIDs do not correspond to a movie due to accidental duplicate entries and/or test entries. Movies are mostly entered by hand, so errors and inconsistencies may exist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read contents of \"movies.dat\" and show sample content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1713::Mouse Hunt (1997)::Children's|Comedy\n",
      "659::Purple Noon (1960)::Crime|Thriller\n",
      "914::My Fair Lady (1964)::Musical|Romance\n",
      "684::Windows (1980)::Drama\n",
      "129::Pie in the Sky (1995)::Comedy|Romance\n",
      "219::Cure, The (1995)::Drama\n",
      "1026::So Dear to My Heart (1949)::Children's|Drama\n",
      "2015::Absent Minded Professor, The (1961)::Children's|Comedy|Fantasy\n",
      "353::Crow, The (1994)::Action|Romance|Thriller\n",
      "1891::Ugly, The (1997)::Horror|Thriller\n"
     ]
    }
   ],
   "source": [
    "val movies_raw = sc.textFile(\"./ml-1m/movies.dat\")\n",
    "movies_raw.takeSample(false,10, seed=0).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert movies data to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val moviesSchema = StructType(Array(StructField(\"movieId\", StringType, true), StructField(\"Title\", StringType, true), StructField(\"Genre\", StringType, true)));\n",
    "val movies_RDD = movies_raw.map(line => Row.fromSeq(line.split(\"::\")))\n",
    "val movies_df = spark.createDataFrame(movies_RDD, moviesSchema)\n",
    "val movies = movies_df.withColumn(\"movieIdTemp\", movies_df(\"movieId\").cast(IntegerType)).drop(\"movieId\").withColumnRenamed(\"movieIdTemp\", \"movieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------+-------+\n",
      "|Title                             |Genre                       |movieId|\n",
      "+----------------------------------+----------------------------+-------+\n",
      "|Toy Story (1995)                  |Animation|Children's|Comedy |1      |\n",
      "|Jumanji (1995)                    |Adventure|Children's|Fantasy|2      |\n",
      "|Grumpier Old Men (1995)           |Comedy|Romance              |3      |\n",
      "|Waiting to Exhale (1995)          |Comedy|Drama                |4      |\n",
      "|Father of the Bride Part II (1995)|Comedy                      |5      |\n",
      "|Heat (1995)                       |Action|Crime|Thriller       |6      |\n",
      "|Sabrina (1995)                    |Comedy|Romance              |7      |\n",
      "|Tom and Huck (1995)               |Adventure|Children's        |8      |\n",
      "|Sudden Death (1995)               |Action                      |9      |\n",
      "|GoldenEye (1995)                  |Action|Adventure|Thriller   |10     |\n",
      "+----------------------------------+----------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the number of movies in the dataset is approximately 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies = 3883\n"
     ]
    }
   ],
   "source": [
    "println(\"Number of movies = \" + movies.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Ratings data into Training (80%) and Test (20%) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2), seed=0L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show resulting Ratings dataset counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ratings = 1000209                                               \n",
      "Training dataset count = 799809, 79.96%                                         \n",
      "Test dataset count = 200400, 20.04%                                             \n"
     ]
    }
   ],
   "source": [
    "val trainingRatio = training.count().toDouble/ratings.count().toDouble*100\n",
    "val testRatio = test.count().toDouble/ratings.count().toDouble*100\n",
    "\n",
    "println(\"Total number of ratings = \" + ratings.count())\n",
    "println(\"Training dataset count = \" + training.count() + \", \" + BigDecimal(trainingRatio).setScale(2, BigDecimal.RoundingMode.HALF_UP).toDouble + \"%\")\n",
    "println(\"Test dataset count = \" + test.count() + \", \" + BigDecimal(testRatio).setScale(2, BigDecimal.RoundingMode.HALF_UP).toDouble+ \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show sample of Ratings Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|    93|   2058|   5.0|\n",
      "|   204|   2424|   3.0|\n",
      "|   555|   1136|   4.0|\n",
      "|   654|   3699|   2.0|\n",
      "|   669|   2693|   5.0|\n",
      "|   724|   2692|   4.0|\n",
      "|   877|   2291|   4.0|\n",
      "|   899|   1270|   4.0|\n",
      "|   973|    246|   5.0|\n",
      "|  1038|   3505|   5.0|\n",
      "+------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.sample(false, 0.0001, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show sample of Ratings Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|   329|   2436|   3.0|\n",
      "|   791|     34|   5.0|\n",
      "|  2000|    110|   5.0|\n",
      "|  2288|   1020|   2.0|\n",
      "|  2386|   2093|   3.0|\n",
      "|  2740|   2153|   1.0|\n",
      "|  3393|   1610|   5.0|\n",
      "|  3445|   3112|   2.0|\n",
      "|  3547|   3260|   3.0|\n",
      "|  4036|      3|   4.0|\n",
      "+------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.sample(false, 0.0001, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the recommendation model on the training data using ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                        (0 + 0) / 10]"
     ]
    }
   ],
   "source": [
    "val als = new ALS().setMaxIter(10).setRegParam(0.01).setUserCol(\"userId\").setItemCol(\"movieId\").setRatingCol(\"rating\")\n",
    "val model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS Model Parameters\n",
    "\n",
    "### Let's take a look at all the paramaters available for ALS, the default values of those parameters which we did not change, and valide the values of those parameters set above.\n",
    "\n",
    "#### We specifically set\n",
    "\n",
    "    MaxIter (maximum number of iterations) = 10 (which is the default)\n",
    "    RegParam (regularization parameter) = 0.01 (default is 0.1)\n",
    "    UserCol (column name for user ids) = “userID” (default is \"user\")\n",
    "    ItemCol (column name for item ids) = “movieID” (default is \"item\")\n",
    "    RatingCol (column for ratings) = “rating” (which is the default)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: alpha for implicit preference (default: 1.0)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations (default: 10)\n",
      "finalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\n",
      "implicitPrefs: whether to use implicit preference (default: false)\n",
      "intermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\n",
      "itemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: movieId)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 10, current: 10)\n",
      "nonnegative: whether to use nonnegative constraint for least squares (default: false)\n",
      "numItemBlocks: number of item blocks (default: 10)\n",
      "numUserBlocks: number of user blocks (default: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "rank: rank of the factorization (default: 10)\n",
      "ratingCol: column name for ratings (default: rating, current: rating)\n",
      "regParam: regularization parameter (>= 0) (default: 0.1, current: 0.01)\n",
      "seed: random seed (default: 1994790107)\n",
      "userCol: column name for user ids. Ids must be within the integer value range. (default: user, current: userId)\n"
     ]
    }
   ],
   "source": [
    "println(als.explainParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model against the Test data and show a sample of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 122:==================================================>  (192 + 7) / 200]+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|  1605|    148|   2.0| 1.8921044|\n",
      "|    53|    148|   5.0| 3.3562677|\n",
      "|  2507|    148|   4.0| 2.9727654|\n",
      "|  3717|    463|   2.0|  3.038925|\n",
      "|  3650|    463|   2.0| 3.2706196|\n",
      "|  3328|    463|   4.0| 2.5866077|\n",
      "|  4252|    463|   3.0| 3.2075772|\n",
      "|   331|    463|   4.0| 2.5212603|\n",
      "|  4040|    463|   1.0| 1.9798533|\n",
      "|  3753|    463|   2.0| 2.0343888|\n",
      "+------+-------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val predictions = model.transform(test).na.drop()\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model by computing the RMSE on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark ML evaluator for regression, RegressionEvaluator, expects two input columns: prediction and label. RegressionEvaluator supports “rmse” (default), “mse”, “r2”, and “mae”. \n",
    "\n",
    "We will use RMSE, which is the square root of the average of the square of all of the error. \n",
    "RMSE is an excellent general purpose error metric for numerical predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 152:=================================================>   (188 + 7) / 200]Root-mean-square error = 0.8938252623053243\n"
     ]
    }
   ],
   "source": [
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\").setLabelCol(\"rating\").setPredictionCol(\"prediction\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(\"Root-mean-square error = \" + rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show that a smaller value of rmse is better\n",
    "\n",
    "This is obviously the case since RMSE is an aggregation of all the error. Thus evaluator.isLargerBetter should be 'false'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.isLargerBetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the Model\n",
    "Build a Parameter Grid specifying what parameters and values will be evaluated in order to determine the best combination.\n",
    "\n",
    "Spark ML algorithms provide many hyperparameters for tuning models. These hyperparameters are distinct from the model parameters being optimized by ML itself. Hyperparameter tuning is accomplished by choosing the best set of parameters based on model performance on test data that the model was not trained with. All combinations of hyperparameters specified will be tried in order to find the one that leads to the model with the best evaluation result.\n",
    "\n",
    "In this example, we will only be evaluating the ALS regularization parameter, regParam. In machine learning, regularization refers to a process of introducing additional information in order to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val paramGrid = new ParamGridBuilder().addGrid(als.regParam, Array(0.01, 0.1)).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cross validator to tune the model with the defined parameter grid\n",
    "\n",
    "Cross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one.\n",
    "\n",
    "In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then combined to produce a single estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cv = new CrossValidator().setEstimator(als).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the parameter grid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\tals_e4383e2e19ae-regParam: 0.01\n",
      "}\n",
      "{\n",
      "\tals_e4383e2e19ae-regParam: 0.1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cv.getEstimatorParamMaps.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-evaluate to find the best model\n",
    "\n",
    "using the RMSE evaluator and hyperparameters specified in the parameter grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 489:>                                                        (0 + 2) / 2]"
     ]
    }
   ],
   "source": [
    "val cvModel = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fit root-mean-square error = 0.8938252623053243                            \n"
     ]
    }
   ],
   "source": [
    "println(\"Best fit root-mean-square error = \" + evaluator.evaluate(cvModel.transform(test).na.drop()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets use our model to recommend movies to a user\n",
    "For this example we will be recommending movies to user with userID = 3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "val userId = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataFrame with the movies that user 3000 has rated\n",
    "First let's take a look at user 3000's ratings in the ratings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|  3000|   2987|   4.0|\n",
      "|  3000|   2990|   3.0|\n",
      "|  3000|   3793|   3.0|\n",
      "|  3000|   1252|   4.0|\n",
      "|  3000|   2997|   4.0|\n",
      "|  3000|   1259|   3.0|\n",
      "|  3000|    589|   4.0|\n",
      "|  3000|      9|   1.0|\n",
      "|  3000|   1265|   5.0|\n",
      "|  3000|    733|   5.0|\n",
      "+------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val movies_watched = ratings.filter(ratings(\"userId\") === userId)\n",
    "movies_watched.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate  user 3000's minimum, maximum and average movie rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 574:============================>                            (1 + 1) / 2]+-----------+-----------+------------------+\n",
      "|min(rating)|max(rating)|       avg(rating)|\n",
      "+-----------+-----------+------------------+\n",
      "|        1.0|        5.0|3.2641509433962264|\n",
      "+-----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_watched.select(min($\"rating\"), max($\"rating\"), avg($\"rating\") ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show user 3000's top 10 rated movies (with movie title, genre, and rating)\n",
    "To do this, we must join the ratings dataset with the movies dataset. The ratings dataset only contains the movieID. Only the movies dataset contains the movie title and genre.\n",
    "\n",
    "We are joining the ratings and movies datasets based on the common movieID column, filtering on userID 3000, and sorting in descending order by rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 578:=====================================>               (142 + 3) / 200]+------+-------+------------------------------+-------------------------------+------+\n",
      "|userId|movieId|Title                         |Genre                          |rating|\n",
      "+------+-------+------------------------------+-------------------------------+------+\n",
      "|3000  |733    |Rock, The (1996)              |Action|Adventure|Thriller      |5.0   |\n",
      "|3000  |3552   |Caddyshack (1980)             |Comedy                         |5.0   |\n",
      "|3000  |2968   |Time Bandits (1981)           |Adventure|Fantasy|Sci-Fi       |5.0   |\n",
      "|3000  |34     |Babe (1995)                   |Children's|Comedy|Drama        |5.0   |\n",
      "|3000  |1197   |Princess Bride, The (1987)    |Action|Adventure|Comedy|Romance|5.0   |\n",
      "|3000  |1307   |When Harry Met Sally... (1989)|Comedy|Romance                 |5.0   |\n",
      "|3000  |590    |Dances with Wolves (1990)     |Adventure|Drama|Western        |5.0   |\n",
      "|3000  |1199   |Brazil (1985)                 |Sci-Fi                         |5.0   |\n",
      "|3000  |1653   |Gattaca (1997)                |Drama|Sci-Fi|Thriller          |5.0   |\n",
      "|3000  |1265   |Groundhog Day (1993)          |Comedy|Romance                 |5.0   |\n",
      "+------+-------+------------------------------+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.as('a).filter(ratings(\"userId\") === userId).join(movies.as('b), $\"a.movieId\" === $\"b.movieId\").select(\"a.userId\", \"a.movieId\", \"b.Title\", \"b.Genre\", \"a.rating\").sort($\"a.rating\".desc).show(10,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining what movies user 3000 has not already watched and rated so that we can make new movie recommendations\n",
    "\n",
    "In order to make new movie recommendations from the list of movies in the movies dataset, we must first figure out which movies user 3000 has not already watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|    68|   2908|   5.0|\n",
      "|   173|   3730|   5.0|\n",
      "|   456|   2917|   2.0|\n",
      "|   526|    589|   4.0|\n",
      "|   533|   2348|   3.0|\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Count = 1000103                                                                 \n"
     ]
    }
   ],
   "source": [
    "val movies_notwatched = ratings.filter(ratings(\"userId\") !== userId)\n",
    "movies_notwatched.sample(false, 0.0001, seed=0).show(5)\n",
    "println(\"Count = \" + movies_notwatched.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining what movies user 3000 has not already watched and rated so that we can make new movie recommendations - another attempt\n",
    "\n",
    "This previous attempt at determining what movies 3000 has not already watched, did not really give us what we need. It simply provided all the movie ratings in the ratings dataset not rated by user 3000. What we want, instead, is a list of all movies in the movies dataset that user 3000 has not rated so that we can make new movie recommendations based on these movies which user 3000 has not yet watched. We don't want to recommend movies that the user has already rated.\n",
    "\n",
    "In order to do this, we will again need to join the ratings and movies datasets. However, unlike the the previous join we did, which was an inner join, this join needs to be an outer join as we want all the movies in the movies dataset that the user has not rated in the ratings dataset. Since the join order is ratings then movies, we specifically need to employ a right outer join. We will again use movieId as the join column, filter on userId 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 582:============================>                            (1 + 1) / 2]+-------+----------------------------------+\n",
      "|movieId|Title                             |\n",
      "+-------+----------------------------------+\n",
      "|1      |Toy Story (1995)                  |\n",
      "|2      |Jumanji (1995)                    |\n",
      "|3      |Grumpier Old Men (1995)           |\n",
      "|4      |Waiting to Exhale (1995)          |\n",
      "|5      |Father of the Bride Part II (1995)|\n",
      "|6      |Heat (1995)                       |\n",
      "|7      |Sabrina (1995)                    |\n",
      "|8      |Tom and Huck (1995)               |\n",
      "|11     |American President, The (1995)    |\n",
      "|12     |Dracula: Dead and Loving It (1995)|\n",
      "+-------+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val movies_notwatched_movieId = ratings.filter(ratings(\"userId\") === userId).as('a).join(movies.as('b), $\"a.movieId\" === $\"b.movieId\", \"right\").filter($\"a.movieId\".isNull).select($\"b.movieId\", $\"b.Title\").sort($\"b.movieId\".asc)\n",
    "movies_notwatched_movieId.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do a check on the DataFrame we just created\n",
    "Let's check to see that the results of our right outer join make sense by looking at the row count of the resulting DataFrame. The number of movies not watched by the user from the movies dataset (the result of the right outer join) plus the number of movies rated by the user in the ratings dataset should equal the total number of movies in the movies dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of movies = 3883\n",
      "Number of movies rated by user = 106                                            \n",
      "Number of movies NOT rated by user = 3777                                       \n"
     ]
    }
   ],
   "source": [
    "println(\"Total number of movies = \" + movies.count())\n",
    "println(\"Number of movies rated by user = \" + ratings.filter(ratings(\"userId\") === userId).count())\n",
    "println(\"Number of movies NOT rated by user = \" + movies_notwatched_movieId.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input DataFrame to use with the model to recommend new movies\n",
    "The ALS algorithm we used above to build the recommendation model requires two inputs for making predictions - userID and movieId. Look back at the ratings dataset we used earlier to test the model to confirm this.\n",
    "\n",
    "Remember that the rating column in the ratings dataset is only used for training and evaluating the model, not for making predicitons.\n",
    "\n",
    "In order to make the DataFrame resulting from the right outer join ready for input into the model to make movie predicitons for user 3000, we need to add a userId column to the DataFrame with userId set to 3000 for every movieId. As this is the dataset that predicitons will be made on, it must contain movieId and userID colums with values for the movies (those which the user has not already watched and rated - the result of the right outer join) and the userId (in this case 3000) for whom we are making recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 599:===================================================> (196 + 2) / 200]+-------+----------------------------------+------+\n",
      "|movieId|Title                             |userId|\n",
      "+-------+----------------------------------+------+\n",
      "|1      |Toy Story (1995)                  |3000  |\n",
      "|2      |Jumanji (1995)                    |3000  |\n",
      "|3      |Grumpier Old Men (1995)           |3000  |\n",
      "|4      |Waiting to Exhale (1995)          |3000  |\n",
      "|5      |Father of the Bride Part II (1995)|3000  |\n",
      "|6      |Heat (1995)                       |3000  |\n",
      "|7      |Sabrina (1995)                    |3000  |\n",
      "|8      |Tom and Huck (1995)               |3000  |\n",
      "|11     |American President, The (1995)    |3000  |\n",
      "|12     |Dracula: Dead and Loving It (1995)|3000  |\n",
      "+-------+----------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val data_userId = movies_notwatched_movieId.withColumn(\"userId\", lit(userId))\n",
    "data_userId.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are now ready to make movie recommendations!!!\n",
    "We will take the DataFrame we just created and run our model against it - outputing the movie title and the rating predicted by the model for user 3000. The results are sorted by predicted rating and limited to the top ten results, so we can give the user ten movie recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 634:=============================================>       (173 + 4) / 200]+--------------------------------------------------+----------+\n",
      "|Title                                             |prediction|\n",
      "+--------------------------------------------------+----------+\n",
      "|Still Crazy (1998)                                |7.2666903 |\n",
      "|Algiers (1938)                                    |7.1671004 |\n",
      "|My Name Is Joe (1998)                             |6.7693667 |\n",
      "|Hell in the Pacific (1968)                        |6.3978605 |\n",
      "|Last Days, The (1998)                             |6.3157787 |\n",
      "|Star Maker, The (Uomo delle stelle, L') (1995)    |6.3082347 |\n",
      "|One Magic Christmas (1985)                        |6.2919774 |\n",
      "|My Best Fiend (Mein liebster Feind) (1999)        |5.9928555 |\n",
      "|Spirits of the Dead (Tre Passi nel Delirio) (1968)|5.991328  |\n",
      "|Office Killer (1997)                              |5.9232535 |\n",
      "+--------------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val predictions_userId = cvModel.transform(data_userId).na.drop()\n",
    "val top10 = predictions_userId.select(\"Title\", \"prediction\").sort($\"prediction\".desc).show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "This notebook was intended to illustrate how to use spark.ml machine learning with DataFrames to create a recommendation system for making movie recommendations to a user. The recommendations are based on that user's prior ratings feedback in relation to those of the entire user community who have rated other movies that we would like to recommend to the user. This notebook also demonstrates how to use the DataFrame API to inspect and transform the input datasets in support of the machine learning approach employed.\n",
    "\n",
    "Please note that although this demo illustrates how to tune the model for better fit, no attempt was made to actually optimize to the best possible model. The intent was simply to show the methodology.\n",
    "\n",
    "This notebook is self standing. That is, all data required to run the notebook is downloaded from within the notebook itself - specifically the \"ratings.dat\" and \"movie.dat\" files from the MovieLens 1M dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![IBM Logo](http://www-03.ibm.com/press/img/Large_IBM_Logo_TN.jpg)\n",
    "\n",
    "Rich Tarro  \n",
    "Big Data Architect, IBM Corporation  \n",
    "email: rtarro@us.ibm.com\n",
    "\n",
    "June 7, 2016  \n",
    "Updated April 23, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark 2.1",
   "language": "scala",
   "name": "scala-spark21"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
